{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawler template.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0L-RfjIQGlXg",
        "QYW7Tz_vW3P9",
        "iYBaJlGT1JUo",
        "V339ekhYYmIv",
        "STWZAWY6YgXc",
        "WeBsiLs9YyV2",
        "NiNzrrDbHeIs"
      ],
      "authorship_tag": "ABX9TyOZ0WkXn5ZuwQXLowAPbUOx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrDobbert/web_scraping/blob/main/Crawler_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwuRBqoGaN1p"
      },
      "source": [
        "!pip install scrapy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBA_AQaXe00B"
      },
      "source": [
        "import time\n",
        "from scrapy import Selector\n",
        "# Import the CrawlerProcess\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "\n",
        "from urllib.parse import urljoin\n",
        "import scrapy\n",
        "# Import requests to load text from URL\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYW7Tz_vW3P9"
      },
      "source": [
        "# CRAWLER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAJsag8K2ykq"
      },
      "source": [
        "# Create the Spider class\n",
        "class MammutSpider(scrapy.Spider):\n",
        "  name = \"MammutSpider\"\n",
        "  \n",
        "  # start_requests method\n",
        "  def start_requests(self):  \n",
        "    urls = [\n",
        "            \"https://www.mammut.com/ch/de/cat/1010/bekleidung/?filter.availability=Available&page=1\"\n",
        "    ]\n",
        "    \n",
        "    for url in urls:\n",
        "      time.sleep(1)\n",
        "      yield scrapy.Request(url = url, callback = self.parse_get_pages)\n",
        "      \n",
        "  def parse_get_pages(self, response):\n",
        "    # Parser\n",
        "    links = response.xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"mm-product__product-url\", \" \" ))]/@href').getall()\n",
        "    for link in links:\n",
        "        yield scrapy.Request(url = link, callback = self.parse_get_products)\n",
        "\n",
        "  def parse_get_products(self, response):\n",
        "    \n",
        "    product_title = response.xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"buy-box__title\", \" \" ))]/text()').extract_first().strip()\n",
        "    product_subtitle = response.xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"buy-box__subtitle\", \" \" ))]/text()').extract_first()\n",
        "    product_prices = response.xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"buy-box__price\", \" \" ))]/text()').extract_first()\n",
        "    product_purpose = response.css('.product-detail-page__purpose-item-icon').xpath('@alt').extract()\n",
        "    product_description = response.css('.product-detail-page__description-text::text').extract()\n",
        "    product_content = response.css('li.product-description__pointed-list-item').extract()\n",
        "    print(product_title)\n",
        "    # get product detail = material and selling points\n",
        "    product_detail = response.css('.product-detail-page').getall()\n",
        "\n",
        "    str_product_detail = str(product_detail)\n",
        "    materialstr = str(product_detail)\n",
        "\n",
        "    sellingPoints = re.findall(r'(?<=sellingPoints\":).*$', str_product_detail)\n",
        "    sellingPoints2 = re.findall('^(.+?)}', sellingPoints[0] )\n",
        "\n",
        "    material = re.findall(r'(?<=\"Material &amp; Technologie\",\"materials\":).*$', materialstr)\n",
        "    material2 = re.findall('^(.+?)}]', material[0] )\n",
        "\n",
        "    mammut_dict[product_title] = [product_title,product_subtitle, product_prices, product_purpose, product_content, product_description, sellingPoints2, material2]\n",
        "\n",
        "#_________________________________________________________________________   \n",
        "# Initialize the dictionary **outside** of the Spider class\n",
        "mammut_dict = dict()\n",
        "#links = []\n",
        "\n",
        "# Run the Spider\n",
        "process = CrawlerProcess()\n",
        "process.crawl(MammutSpider)\n",
        "process.start()\n",
        "\n",
        "\n",
        "# Print a preview of content\n",
        "print(mammut_dict)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm0eyqGdTgbn",
        "outputId": "2a9c5186-cd0e-4b57-d43e-8fdd12f54b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(mammut_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V339ekhYYmIv"
      },
      "source": [
        "# Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OEjt1lUYcRy"
      },
      "source": [
        "\n",
        "_____________________________________\n",
        "\n",
        "# First parsing method\n",
        "  def parse_front(self, response):\n",
        "    course_blocks = response.css('div.course-block')\n",
        "    course_links = course_blocks.xpath('./a/@href')\n",
        "    links_to_follow = course_links.extract()\n",
        "    for url in links_to_follow:\n",
        "      yield response.follow(url = url,\n",
        "                            callback = self.parse_pages)\n",
        "# Second parsing method\n",
        "  def parse_pages(self, response):\n",
        "    \n",
        "    # Create a SelectorList of the course titles text\n",
        "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
        "    \n",
        "    # Extract the text and strip it clean\n",
        "    crs_title_ext = crs_title.extract_first().strip()\n",
        "    \n",
        "    # Create a SelectorList of course descriptions text\n",
        "    crs_descr = response.css( 'p.course__description::text' )\n",
        "    \n",
        "    # Extract the text and strip it clean\n",
        "    crs_descr_ext = crs_descr.extract_first().strip()\n",
        "   \n",
        "    # Fill in the dictionary\n",
        "    dc_dict[crs_title_ext] = crs_descr_ext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeBsiLs9YyV2"
      },
      "source": [
        "# CSV writer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7syO-2NQ4gC"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open(\"crawler_output.csv\", \"w\", newline=\"\") as csvfile: \n",
        "  articlewriter = csv.writer(csvfile, delimiter=';', quotechar='\"',quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "  for article in articles:\n",
        "    articlewriter.writerow([article.emoji, article.title, article.image, article.content])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw1AWpsbckLA"
      },
      "source": [
        "# SELECTOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_eU1S-IXBGa"
      },
      "source": [
        "# Import a scrapy Selector\n",
        "from scrapy import Selector\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "# Import requests to load text from URL\n",
        "import requests\n",
        "import re\n",
        "\n",
        "\n",
        "links = []\n",
        "\n",
        "url_dict = {\n",
        "  \"men_url\": \"https://www.mammut.com/ch/de/cat/1010/bekleidung/?filter.availability=Available&page=%s\",\n",
        "  \"women_url\": \"https://www.mammut.com/ch/de/cat/2010/bekleidung/?filter.availability=Available&page=%s\"  \n",
        "}\n",
        "\n",
        "#url_dict.get(\"men_url\")\n",
        "\n",
        "def get_links(main_url):\n",
        "\n",
        "  html = requests.get( main_url % 1 ).content\n",
        "\n",
        "  # Create the Selector object sel from html\n",
        "  sel = Selector( text = html )\n",
        "\n",
        "  # Extract from selector\n",
        "  next_page = sel.css('.mm-product-list-page__pagination').extract()\n",
        "  str_x = str(next_page)\n",
        "  next_page_2 = re.findall(r'(?<=init-pages-quantity=).*$', str_x)\n",
        "  last_page = (int(next_page_2[0].split()[0].replace('\"',\"\")))\n",
        "\n",
        "  for i in range(1,last_page+1):\n",
        "    time.sleep(1)\n",
        "    url = main_url % i\n",
        "    print(url)\n",
        "    html = requests.get( url ).content\n",
        "    sel = Selector( text = html )\n",
        "    products = sel.xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"mm-product__product-url\", \" \" ))]/@href').getall()\n",
        "    for i in products:\n",
        "      links.append(urljoin(mammut,i))\n",
        "  \n",
        "get_links(url_dict.get(\"men_url\"))  \n",
        "get_links(url_dict.get(\"women_url\"))\n",
        "\n",
        "print(len(links))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-FbgOkyRYrZ",
        "outputId": "bd36057a-305b-42d6-b9c9-cff93b850084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#print(links)\n",
        "\n",
        "x = \"https://www.mammut.com/ch/de/cat/1010/bekleidung/?filter.availability=Available&page=1\"\n",
        "\n",
        "if \"2010\" in x:\n",
        "  gender = \"female\"\n",
        "elif \"1010\" in x:\n",
        "  gender = \"male\"\n",
        "\n",
        "print(gender)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "male\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwIc_56qf5nb"
      },
      "source": [
        "import re\n",
        "product_list = []\n",
        "x = 0\n",
        "product_list.append([\"product_title\", \"product_subtitle\", \"gender\", \"product_price\",\"product_price_old\",\"sale_type\", \"product_description\"])\n",
        "urls = links\n",
        "for url in urls:\n",
        "  x = x + 1\n",
        "  print(x)\n",
        "  #time.sleep(1)\n",
        "  if \"women\" in url:\n",
        "    gender = \"women\"\n",
        "  elif \"men\" in url:\n",
        "    gender = \"men\"\n",
        "  else:\n",
        "    gender = \"unisex\"\n",
        "  print(gender)\n",
        "  \n",
        "  html = requests.get( url ).content\n",
        "  sel = Selector( text = html )\n",
        "  product_title = sel.xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"buy-box__title\", \" \" ))]/text()').extract_first().strip()\n",
        "  product_subtitle = sel.xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"buy-box__subtitle\", \" \" ))]/text()').extract_first()\n",
        "  product_price = sel.xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"buy-box__prices\", \" \" ))]/div[1]/text()').extract_first().replace(\"CHF \",\"\")\n",
        "  product_price_old = sel.xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"buy-box__prices\", \" \" ))]/div[2]/text()').extract_first()\n",
        "  if product_price_old == None:\n",
        "    product_price_old = 0\n",
        "  else:\n",
        "    product_price_old = product_price_old.replace(\"CHF \",\"\")\n",
        "  sale_type = sel.xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"product-badge product-badge--label product-detail-page__badge\", \" \" ))]/text()').extract_first()\n",
        "  print(sale_type)\n",
        "  product_description = sel.css('.product-detail-page__description-text::text').extract()\n",
        "  product_description = product_description[0]\n",
        "  #product_content = sel.css('li.product-description__pointed-list-item').extract()\n",
        "  # get product detail = material and selling points\n",
        "  #product_detail = sel.css('.product-detail-page').getall()\n",
        "  #str_product_detail = str(product_detail)\n",
        "  #materialstr = str(product_detail)\n",
        "  #sellingPoints = re.findall(r'(?<=sellingPoints\":).*$', str_product_detail)\n",
        "  #sellingPoints2 = re.findall('^(.+?)}', sellingPoints[0] )\n",
        "  #material = re.findall(r'(?<=\"Material &amp; Technologie\",\"materials\":).*$', materialstr)\n",
        "  #material2 = re.findall('^(.+?)}]', material[0] )\n",
        "  #product_list.append([product_title,product_subtitle, product_prices, product_purpose, product_content, product_description, sellingPoints2, material2])\n",
        "  product_list.append([product_title, product_subtitle, gender, product_price, product_price_old, sale_type, product_description])\n",
        "#print(product_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdvpbVBLjhVD",
        "outputId": "a8f0361b-91a2-4281-9c91-84f097d7cda9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#print(product_list)\n",
        "print(len(product_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdOsuCxQ4hhK",
        "outputId": "bab1a733-0a45-4074-a964-d5fec24e669f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(product_list[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['product_title', 'product_subtitle', 'gender', 'product_price', 'product_price_old', 'sale_type', 'product_description'], ['Sloper', 'T-Shirt für Herren', 'male', '55.00', 0, None, 'Sloper T-Shirt Men. Kletter-T-Shirt mit angesagtem Print. Das Oberteil kommt in einer weichen bioRe®-Baumwollqualität und wurde unter ökologischen Richtlinien sowie unter fairen Arbeitsbedingungen produziert.'], ['Convey Tour', 'Hardshell-Jacke mit Kapuze für Herren', 'male', '280.00', 0, None, 'Convey Tour HS Hooded Jacket Men. Die Regenjacke für absoluten Wetterschutz und mit allen Funktionen einer Wanderjacke. Wasserdicht. Winddicht. Atmungsaktiv. Dank GORE-TEX® Paclite. Der hochwertige und leichte Stoff macht die Hardshelljacke besonders bequem. Dank Easy Combine lässt sich die Jacke mit unterschiedlichen Isolations-Jacken und Midlayern kombinieren. Mehr Komfort. Mehr Leichtigkeit. Mehr Performance. Mit der Convey Tour HS Hooded Jacket Men.'], ['Zinal Guide', 'Hosen für Herren', 'male', '220.00', 0, None, 'Zinal Guide Pants Men. Die vollausgestattete Trekkinghose für längere Wandertouren. Robuster Stoff mit viel Bewegungsfreiheit dank ausgewähltem Material und speziellem Fit. Hochfunktionell: viele Taschen, Beinbelüftung und -verstellung. Lässt keine Wanderwünsche offen: die Zinal Guide Pants Men.'], ['Ultimate V', 'Softshell-Jacke mit Kapuze für Herren', 'male', '330.00', 0, None, 'Du suchst für deine nächste Wanderung eine multifunktionale Jacke? Die Ultimate V SO Hooded Jacket Men bietet dir alles, was du für wechselhaftes Wetter in den Bergen benötigst. Dank ganzjährig einsetzbarem, exklusiv mit GORE® entwickeltem 3-Lagen PFC-freiem GORE® WINDSTOPPER®-Material ist diese Softshell-Jacke rundum winddicht und schützt dich vor kalten Böen. Ausserdem kannst du dich hinter Wind- und Kinnschutz verstecken. Daneben macht die DWR-Behandlung des Aussenstoffes die Jacke wasserabweisend und resistent gegen Regen und Nässe. Die Jacke ist mit einer Membran ausgestattet, die sie zusätzlich atmungsaktiv macht. Auf diese Weise wird gewährleistet, dass du nach einem schweisstreibenden Teil deiner Wanderung weder auskühlst, noch ein Hitzestau unter deiner Kleidung entstehen kann. Das leichte Backing mit offener 3D-Struktur steht für einen optimalen Feuchtigkeitstransport. Das Wicking Finish auf dem Backing absorbiert Feuchtigkeit von der Haut und beschleunigt den Trocknungsprozess. Diese Funktionen machen dich fit für die nächste Wanderung bei Wind und Wetter!']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0j9i6Ikfa3X"
      },
      "source": [
        "# Create spreadsheet in google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVr7FZTb7dKq"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "#sh = gc.create('A new spreadsheet')\n",
        "\n",
        "# Open our new sheet and add some data.\n",
        "wks = gc.open('A new spreadsheet').sheet1\n",
        "\n",
        "for entry in product_list[501:]:\n",
        "  #print(entry)\n",
        "  wks.append_row(entry)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLpv_GItCe53"
      },
      "source": [
        "# get_all_values gives a list of rows.\n",
        "rows = wks.get_all_values()\n",
        "print(rows)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}